{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Emotion with Azure Cognitive Services\n",
    "\n",
    "Azure Cognitive Services (ACS) provides an Application Programming Interface (API) for analyzing images and detecting the emotions on faces in those images. This notebook uses that service to produce a report of the dominant emotion detected on each face in any image provided by a link to that image.\n",
    "\n",
    "## Imports\n",
    "\n",
    "First, you must import the packages necessary for this demonstration. You can run code in this notebook by pressing `Shift`+`Enter` when your cursor is in the cell. We will be doing this a lot in this demonstration. Try it by clicking in the cell below and running the code. It should print \"All imports were successful\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ‰All imports were successful!ðŸŽ‰\n"
     ]
    }
   ],
   "source": [
    "import requests                        # for communicating with Azure\n",
    "\n",
    "import json                            # for understanding the JavaScript Object Notation (JSON) data from Azure\n",
    "\n",
    "%matplotlib inline                \n",
    "import matplotlib.pyplot as plt        # for displaying the image with faces and their emotion highlighted\n",
    "from matplotlib import patches\n",
    "\n",
    "from io import BytesIO                 # for parsing the target image to bytes\n",
    "\n",
    "from IPython.core.display import HTML  # for displaying a table with the raw Azure response\n",
    "\n",
    "print(\"ðŸŽ‰All imports were successful!ðŸŽ‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Configuration Settings from `config.json`\n",
    "\n",
    "> If you haven't set up `config.json`, see README.md instructions 8-10 for details.\n",
    "\n",
    "This function opens the `config.json` file and reads a value found for key given. If you get a `FileNotFoundError` or a `JSONDecodeError`. Double check that you have properly saved the `config.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config(key: str) -> str:\n",
    "    # Opens `config.json` and returns the value corresponding to the `key`.\n",
    "    with open(\"config.json\",\"r\") as config:    # open the config.json file\n",
    "        return json.loads(config.read())[key]  # read the value at the key and return it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sets the API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_url = config(\"endpoint\") + \"/detect\"  # get the value corresponding to the `endpoint` key, appends \"/detect\"\n",
    "api_key = config(\"keys\")[0]                    # get the first value corresponding to the `keys` key "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Example Images\n",
    "\n",
    "I have selected some example images for you to test emotion recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_0 = \"https://i.imgur.com/aegNK4W.jpg\"\n",
    "example_1 = \"https://i.imgur.com/RLSN9rx.jpg\"\n",
    "example_2 = \"https://i.imgur.com/vEuLg5n.jpg\"\n",
    "\n",
    "royalty = \"https://peopledotcom.files.wordpress.com/2018/12/duchess-cambridge-xmas-card-2000.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Emotion Analysis Function\n",
    "\n",
    "The function below uses the [`requests`](https://2.python-requests.org/en/master/) package to make an HyperText Transfer Protocol (HTTP) call to the ACS API. It returns a [`JSON`](https://www.w3schools.com/whatis/whatis_json.asp) with the position of faces in the image and their emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image(image_url: str):\n",
    "    # Returns face positions and their emotions as json by calling the Azure Cognitive Services Face Recognition API\n",
    "    response = requests.post(                            # create a request to Azure with\n",
    "        endpoint_url,                                    # the Azure service url to contact\n",
    "        params={\"returnFaceAttributes\": \"emotion\"},      # the face attributes you want \n",
    "        headers={\"Ocp-Apim-Subscription-Key\": api_key},  # the api key to authenticate your request\n",
    "        json={\"url\": image_url},                         # and the image you want to analyze\n",
    "    )\n",
    "    return response.json()                               # return what the API gives you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to see the image and raw output returned from the API. Change the `image_url` to an image of your choice to see what ACS returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table>\n",
       "    <tr>\n",
       "        <th style=\"text-align:center\">Image</th>\n",
       "        <th style=\"text-align:left\">Response from Azure</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"vertical-align:top\"><img src=\"https://i.imgur.com/vEuLg5n.jpg\" style=\"width:400px\"/></td>\n",
       "        <td style=\"text-align:left; vertical-align:top\"><pre>[\n",
       "    {\n",
       "        \"faceId\": \"2326b949-6389-4f5c-b994-666fec4955fe\",\n",
       "        \"faceRectangle\": {\n",
       "            \"top\": 401,\n",
       "            \"left\": 323,\n",
       "            \"width\": 447,\n",
       "            \"height\": 447\n",
       "        },\n",
       "        \"faceAttributes\": {\n",
       "            \"emotion\": {\n",
       "                \"anger\": 0.0,\n",
       "                \"contempt\": 0.0,\n",
       "                \"disgust\": 0.0,\n",
       "                \"fear\": 0.001,\n",
       "                \"happiness\": 0.0,\n",
       "                \"neutral\": 0.0,\n",
       "                \"sadness\": 0.0,\n",
       "                \"surprise\": 0.999\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "]</pre></td>\n",
       "    </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_url = example_2             # set the image to analyze\n",
    "faces = analyze_image(image_url)  # analyze the image\n",
    "\n",
    "# Display the image and raw JSON response in a table\n",
    "display(HTML(f\"\"\"\n",
    "<table>\n",
    "    <tr>\n",
    "        <th style=\"text-align:center\">Image</th>\n",
    "        <th style=\"text-align:left\">Response from Azure</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"vertical-align:top\"><img src=\"{image_url}\" style=\"width:400px\"/></td>\n",
    "        <td style=\"text-align:left; vertical-align:top\"><pre>{json.dumps(faces, indent=4)}</pre></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate the Images\n",
    "\n",
    "This function makes use of the `analyze_image` function from above to plot the image with and overlay of faces and the dominant emotion detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dominant_emotion(emotions: dict) -> str:\n",
    "    # Returns the emotion with the highest rating of correctness from the emotions dictionary\n",
    "    return max(emotions, key=emotions.get).capitalize()\n",
    "\n",
    "\n",
    "def get_face_patch(face_rectangle):\n",
    "    # Returns a rectangular patch that can be overlaid on a face found in the target image \n",
    "    origin = (face_rectangle[\"left\"], face_rectangle[\"top\"])\n",
    "    width, height = face_rectangle[\"width\"], face_rectangle[\"height\"]\n",
    "    return patches.Rectangle(origin, width, height, fill=False, linewidth=2, color=\"w\")\n",
    "\n",
    "\n",
    "def annotate_faces(image_url):\n",
    "    # Plots the target image with an overlay of rectangles around the faces found by Azure Cognitive Services\n",
    "    \n",
    "    # Analyze the image with ACS\n",
    "    faces = analyze_image(image_url)\n",
    "\n",
    "    # Save the image for plotting\n",
    "    image_file = BytesIO(requests.get(image_url).content)\n",
    "    image = Image.open(image_file)\n",
    "\n",
    "    # Plot the base image\n",
    "    plt.figure(figsize=(18, 18))\n",
    "    plt.text(0, 0, f\"{len(faces)} faces\", fontsize=18, backgroundcolor=\"w\")\n",
    "    plt.axis(\"off\")\n",
    "    ax = plt.imshow(image, alpha=0.8)\n",
    "\n",
    "    # Annotate each face returned by Azure Cognitive Services\n",
    "    for face in faces:\n",
    "        ax.axes.add_patch(get_face_patch(face[\"faceRectangle\"]))\n",
    "        dominant_emotion = get_dominant_emotion(face[\"faceAttributes\"][\"emotion\"])\n",
    "        left_anchor, top_anchor = (\n",
    "            face[\"faceRectangle\"][\"left\"],\n",
    "            face[\"faceRectangle\"][\"top\"],\n",
    "        )\n",
    "        plt.text(\n",
    "            left_anchor, top_anchor, dominant_emotion, fontsize=18, backgroundcolor=\"w\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it Yourself\n",
    "\n",
    "You can run the code to produce anotated images by calling `annotate_faces` with an image url as an argument. Try it with any image url you find!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotate_faces(example_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Other Cognitive Services API\n",
    "\n",
    "Explore the [directory](https://azure.microsoft.com/en-us/services/cognitive-services/directory/) of Cognitive Service APIs to learn more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.6.7
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
